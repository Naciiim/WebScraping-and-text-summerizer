{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour telecharger la dataset, il faut demander un lien unique\n",
    "# a travers cette forme: https://cornell.qualtrics.com/jfe/form/SV_6YA3HQ2p75XH4IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques statistiques faits\n",
    "----- First 100k articles and summaries from dev: -----\n",
    "Avg. article length (words) 551.2604\n",
    "Avg. summary length (words):  33.9256\n",
    "Avg. article length (sentences) 32.3397\n",
    "Avg. summary length (sentences):  1.8956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVdQK_6flOvg",
    "outputId": "4eb4606f-e379-4d57-e239-197faaed65ab"
   },
   "outputs": [],
   "source": [
    "! pip install wget datasets transformers rouge-score textacy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_key = # code d'acces pour enregsitrer les graphes\n",
    "hf_token = # code d'acces pour uploader le modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=wandb_key)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "proj = \"big  modelof\" + str(now.hour) +'-'+ str(now.minute)\n",
    "# Initialize a wandb run\n",
    "wandb.init(project=proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJWDsivpFEtN",
    "outputId": "9498720b-09bd-4701-e770-a358be9fd906"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from wget import download\n",
    "\n",
    "if \"release\" not in os.listdir():\n",
    "    \n",
    "    #downloading dataset\n",
    "    if \"newsroom-release.tar\" not in os.listdir():\n",
    "        print(\"* Downloading Dataset\")\n",
    "        download(\"https://lil.nlp.cornell.edu/resources/newsroom/r8625bda324/newsroom-release.tar\")\n",
    "    \n",
    "    #unzipping tar file\n",
    "    print(\"\\n* Unzipping dataset\")\n",
    "    os.system(\"tar xvf newsroom-release.tar\")\n",
    "#unzipping gz archives\n",
    "if \"test.jsonl\" not in os.listdir(\"release\"):\n",
    "    print(\"\\n* Unzipping release files\")\n",
    "    os.system(\"gzip -d release/test.jsonl.gz\")\n",
    "    os.system(\"gzip -d release/train.jsonl.gz\")\n",
    "    os.system(\"gzip -d release/dev.jsonl.gz\")\n",
    "    print(\"\\n++ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_SILENT']=\"true\" # turn off wandb warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm  newsroom-release.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_density(df_abs,df_mix,df_ext,r_abs,r_mix,r_ext): # r_abs ratio for abstractive split\n",
    "    # cette fonction prend 3 splits et les fractions a donner a \n",
    "    # chaque split et les repartis en fonction de ces fractions\n",
    "    # par exemple 80% abstractif + 20% extractif + 30% mixte\n",
    "    n_abs = len(df_abs);n_mix = len(df_mix);n_ext = len(df_ext)\n",
    "    n_total = int(n_abs / r_abs) # we use # of abstractive rows as reference\n",
    "    #fractions to take from each\n",
    "    frac_mix = r_mix * n_abs /(r_abs * n_mix)\n",
    "    frac_ext = r_ext * n_abs /(r_abs * n_ext)\n",
    "    #take needed rows(randomly)\n",
    "    ret_mix = df_mix.sample(frac = frac_mix)\n",
    "    ret_ext = df_ext.sample(frac = frac_ext)\n",
    "    #concatenate&return\n",
    "    ret_df = pd.concat([df_abs,ret_mix,ret_ext])\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QM3lWcC3Xv_V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#split settings\n",
    "train_size = 300000\n",
    "split_abs = 0.5\n",
    "split_mix = 0.3\n",
    "split_ext = 0.2\n",
    "\n",
    "df_train = pd.read_json(r'release/train.jsonl', lines=True, nrows=train_size,keep_default_dates=False)\n",
    "\n",
    "df_abs = df_train.loc[df_train[\"density_bin\"]==\"abstractive\"]\n",
    "df_mix = df_train.loc[df_train[\"density_bin\"]==\"mixed\"]\n",
    "df_ext = df_train.loc[df_train[\"density_bin\"]==\"extractive\"]\n",
    "#randomise and get splits \n",
    "df_train_final = split_by_density(df_abs,df_mix,df_ext,split_abs,split_mix,split_ext)\n",
    "\n",
    "\n",
    "n = len(df_train_final)\n",
    "\n",
    "#80% train 20% validation\n",
    "dev_size = int((n/0.8) * 0.2)\n",
    "df_dev = pd.read_json(r'release/dev.jsonl', lines=True, nrows=dev_size,keep_default_dates=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myO_40B9MnJ3",
    "outputId": "9a47b49e-8541-4f34-e8ae-d9d3415feffc"
   },
   "outputs": [],
   "source": [
    "print(\"ratio abstractive: \",len(df_train_final[df_train_final[\"density_bin\"]==\"abstractive\"])/n)\n",
    "print(\"ratio mixed: \",len(df_train_final[df_train_final[\"density_bin\"]==\"mixed\"])/n)\n",
    "print(\"ratio extractive: \",len(df_train_final[df_train_final[\"density_bin\"]==\"extractive\"])/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5bfREdYMD9m"
   },
   "outputs": [],
   "source": [
    "# keep only needed columns (text & summary) to fasten training\n",
    "df_train_final = df_train_final[['text', 'summary']]\n",
    "df_dev = df_dev[['text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJIq3tThMe3d",
    "outputId": "7927ed9b-5733-4b34-c972-20af0fc06cb8"
   },
   "outputs": [],
   "source": [
    "len(df_dev)/len(df_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L67cb2aT0llP",
    "outputId": "47235c66-0dc8-4b74-95f0-a0e2025580e4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "# verification si les gpus sont detectes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8At5HiLqo_P"
   },
   "outputs": [],
   "source": [
    "#adding index to help with training\n",
    "df_train_final.reset_index(inplace=True)\n",
    "df_dev.reset_index(inplace=True)\n",
    "\n",
    "#creation de l objet datasets\n",
    "from datasets import Dataset,load_metric\n",
    "raw_train_ds = Dataset.from_pandas(df_train_final)\n",
    "raw_dev_ds = Dataset.from_pandas(df_dev)\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzGxY49U2J5m"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "# Create a DatasetDict object that contains the two datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": raw_train_ds,\n",
    "    \"validation\": raw_dev_ds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvGDfudJo6QA",
    "outputId": "7a6b8e70-4fd3-40da-8330-ecc6dcb85004"
   },
   "outputs": [],
   "source": [
    "dataset[\"validation\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLFJC5_wnato"
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration,BartTokenizerFast,DataCollatorForSeq2Seq,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "\n",
    "# chargement de modele a entrainer\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)#,return_dict=True)\n",
    "tokenizer = BartTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre traitement\n",
    "from textacy import preprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.make_pipeline( # cleaning pipeline\n",
    "  partial(preprocessing.replace.urls, repl=\"\"),\n",
    "  preprocessing.normalize.bullet_points,\n",
    "  preprocessing.normalize.hyphenated_words,\n",
    "  preprocessing.normalize.quotation_marks,\n",
    "  preprocessing.normalize.unicode,\n",
    "  preprocessing.normalize.whitespace,\n",
    "  preprocessing.remove.accents,\n",
    "  preprocessing.remove.brackets,\n",
    "  preprocessing.remove.html_tags,\n",
    "  partial(preprocessing.replace.emails, repl=\"\"),\n",
    "  partial(preprocessing.replace.emojis, repl=\"\"),\n",
    "  partial(preprocessing.replace.hashtags, repl=\"\"),\n",
    "  )\n",
    "\n",
    "def batch_clean(examples):\n",
    "  for i in range(len(examples)):\n",
    "    examples[\"text\"][i] = preproc(examples[\"text\"][i])\n",
    "    examples[\"summary\"][i] = preproc(examples[\"summary\"][i])\n",
    "  return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKgD9h77nxXf"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "#fonction qui tokenize le jeu de donnees\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDMQ4XnoIlV0"
   },
   "outputs": [],
   "source": [
    "#pretraitement\n",
    "dataset[\"train\"] = dataset[\"train\"].map(batch_clean,batched=True)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(batch_clean,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][199][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Y1EdVl2oKz9",
    "outputId": "9fee7a4d-d64d-47e6-dcd0-9ce9136664f3"
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenized_train_ds = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_dev_ds = dataset[\"validation\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXynLPQvtV5_"
   },
   "outputs": [],
   "source": [
    "#specifique à colab;\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evx48seP4-t2"
   },
   "outputs": [],
   "source": [
    "def better_join(sent_list,sep = '\\n'):\n",
    "  n = len(sent_list)\n",
    "  i = 0\n",
    "  text = ''\n",
    "  while i < n-1:\n",
    "      text = text + str(sent_list[i]) + sep\n",
    "      i = i+1\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q8trmesqXlF"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# fonction utilisée en evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True) # tenseur de tokens -> texte\n",
    "    # on efface -100 dans les labels vu qu on pas les decoder (element non trouvé dans le corpus)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # tenseur de tokens -> texte\n",
    "    \n",
    "    # structuration du texte: rouge necessite \\n entre les phrases\n",
    "    decoded_preds = [better_join(list(nlp(pred.strip()).sents)) for pred in decoded_preds]\n",
    "    decoded_labels = [better_join(list(nlp(label.strip()).sents)) for label in decoded_labels]\n",
    "    # calcul de rouge\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # on extrait les resultats les plus importants (fmeasure du mid de chaque rouge)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # calcul de la longueur du texte generé\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5jYdPZG6bMA",
    "outputId": "71417455-5afa-41b4-91c3-267d57adf70b"
   },
   "outputs": [],
   "source": [
    "# exemple de rouge\n",
    "\n",
    "decoded_labels = ['hello mister.I am very healthy.Are you very healty? Thank you very much',\n",
    "                 'Hello mister.I am very healthy.Are you very healty?',\n",
    "                 'Hello mister.I am very healthy.Are you very healty?Thank you very much. I really like it']\n",
    "                 \n",
    "decoded_preds = ['hello.I am good.Are you? Thanks',\n",
    "                  'hello.I am good.Are you?',\n",
    "                  'hello.I am good.Are you?Thanks.I like it']\n",
    "\n",
    "\n",
    "import pprint\n",
    "def pretty(t):\n",
    "  pp = pprint.PrettyPrinter(width=100,indent=2)\n",
    "  pp.pprint(t)\n",
    "\n",
    "\n",
    "decoded_preds = [better_join(list(nlp(pred.strip()).sents)) for pred in decoded_preds]\n",
    "decoded_labels = [better_join(list(nlp(label.strip()).sents)) for label in decoded_labels]\n",
    "\n",
    "\n",
    "pretty(metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxjXEbvloyS6"
   },
   "outputs": [],
   "source": [
    "# batching\n",
    "batch_size = 10\n",
    "\n",
    "#hyperparametres d entrainement\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./mymodel_v2_2', # non du modele\n",
    "    save_strategy = \"epoch\",\n",
    "    report_to=\"wandb\", # wandb pour l enregistrement des donnees et creation des graphes\n",
    "    hub_strategy = 'checkpoint', # uploads every save_strategy\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 4000, # evaluation chaque 4k etapes\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=batch_size, # activation du batching sur les 2 gpus\n",
    "    per_device_eval_batch_size=batch_size,  # activation du batching sur les 2 gpus\n",
    "    weight_decay=0.01, # le weight decay (on utilise adamw pas adam)\n",
    "    num_train_epochs=2,\n",
    "    fp16=True, # nombre flottants = 16bits\n",
    "    predict_with_generate=True, # pour generer un texte durant la validation\n",
    "    push_to_hub = True, # on upload a la fin\n",
    "    hub_token = hf_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls mymodel_v2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp7w96fxuKV6"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer,model=model) # collateur des donnees\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_dev_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbk3fAxO_MLP",
    "outputId": "80ed65b4-8604-4e09-f9fc-94f3ead14caa"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls mymodel_v2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRIlHJWt6wae",
    "outputId": "cd8c7d62-08a6-48cd-9af7-a1d16a187d76"
   },
   "outputs": [],
   "source": [
    "result = trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache() #vider memoire gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrEssjRXQHo_",
    "outputId": "5ed3ce2c-96f9-4e7c-89c5-d36c8a3da8db"
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3Mnwa8KGBEo"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a090ZUxzLQ8w"
   },
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(raw_dev_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p2lIKyELhYI",
    "outputId": "eef3b553-59c4-49f0-a4f3-a98ba5fd86f5"
   },
   "outputs": [],
   "source": [
    "raw_dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo2EbT4MLbmZ",
    "outputId": "68eead04-1283-47df-b2f4-9d7fe6e82fd3"
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M2sDkr9KcJ5"
   },
   "outputs": [],
   "source": [
    "# test du modele\n",
    "ARTICLE = \"\"\"\n",
    "Former NBCUniversal advertising chief Linda Yaccarino will become Twitter’s new CEO, current chief executive Elon Musk says in a tweet, as the social media platform seeks to reverse a plunge in advertisement revenues.\n",
    "“I am excited to welcome Linda Yaccarino as the new CEO of Twitter!” Musk said on Friday. “@LindaYacc will focus primarily on business operations, while I focus on product design & new technology.”\n",
    "Yaccarino, who modernised the advertising business at the Comcast Corp entertainment and media division, will have her hands full as she takes on a firm loaded with debt, among other challenges.\n",
    "Since Musk acquired Twitter in October, advertisers have fled the platform, worried that their ads could appear next to inappropriate content after the company lost nearly 80 percent of its staff. Musk this year acknowledged that Twitter suffered a massive decline in advertisement revenue.\n",
    "Musk has axed thousands of employees, rushed the launch of a subscription product that allowed scammers to impersonate major brands and suspended users with whom he disagreed.\n",
    "Twitter’s “trajectory will immediately take a 180-degree turn” under Yaccarino’s leadership, said Lou Paskalis, a longtime advertisement industry executive and CEO of AJL Advisory, a marketing consultancy.\n",
    "“I think [Yaccarino] has climbed every mountain she could at NBCU and did it impeccably well, and there’s no greater challenge than restoring order at Twitter,” he said.\n",
    "Yaccarino could not be reached for comment.\n",
    "While Musk said Yaccarino would help build an “everything app”, which he has previously said could offer a variety of services such as peer-to-peer payments, his selection of an advertising veteran signalled that digital ads would continue to be a core focus of the business.\n",
    "In order to diversify revenue sources, Musk has focused on Twitter Blue, a subscription feature that costs users $8 per month to verify their accounts, but the product has had limited success.\n",
    "Independent researcher Travis Brown, who has been tracking the number of Twitter Blue subscribers over time, estimated there were 619,858 customers as of April 30.\n",
    "Yaccarino’s exit is another big blow to NBCUniversal. Last month, NBC parent Comcast said NBCUniversal CEO Jeff Shell was leaving after acknowledging an inappropriate relationship with a woman in the company after a complaint that prompted an investigation.\n",
    "Advertising President Mark Marshall will step in as interim chairman of NBCUniversal’s advertising and partnerships group.\n",
    "Yaccarino joined NBCU in 2011 after 15 years at Turner Entertainment and has been credited with taking the network’s advertisement sales operation into the digital era.\n",
    "As broadcast television audiences migrated to streaming, she took to the stage at Radio City Music Hall last year to tell advertisers their brand messages were not an afterthought. She said NBCUniversal incorporated advertisements in its Peacock streaming service from the outset.\n",
    "“Twitter needs credibility with the advertising community,” said Greg Kahn, chief executive of GK Digital Ventures media consultancy. “Linda has demonstrated her trust, her innovative nature of bringing new partners to the table and a deep bench of relationships.”\n",
    "Musk, the CEO of electric vehicle maker Tesla, completed his purchase of Twitter in October for $44bn. He said in December that he would step aside as CEO once he found “someone foolish enough to take the job”.\n",
    "On Thursday, Musk tweeted that he had found a CEO without naming Yaccarino. One person close to Yaccarino said Musk’s tweet may well have accelerated the timetable for her to join Twitter, which would be a balm to Tesla shareholders.\n",
    "Shares of Tesla were down 1.3 percent on Friday even as analysts commented that a CEO hire would allow Musk to concentrate on the electric car business. Comcast shares were little changed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9AJLzZfBqTe",
    "outputId": "51ac30eb-c23d-42da-ba49-39800782dd9c"
   },
   "outputs": [],
   "source": [
    "len(tokenizer.encode(ARTICLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRV_Y9ejKeVU"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"./mymodel_v2_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wxkyjpnMT4_",
    "outputId": "7c6ebe3e-2cdd-4307-f865-a39e77f3cc08"
   },
   "outputs": [],
   "source": [
    "print(summarizer(ARTICLE, max_length=128, do_sample=True)[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
